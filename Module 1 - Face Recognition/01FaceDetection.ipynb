{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Part 1 Face detection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viola-Jones Algorithm:\n",
    "`Training` and `Detection`<br>\n",
    "### Detection:\n",
    "`Grey Scale` conversion and Feature detection, when feature is found the feature is transfered to original image.<br> \n",
    "<b>fig. Viola-Jones Algorithm after n iterations to find the features</b>\n",
    "![Screenshot%20from%202018-08-24%2011-07-30.png](Screenshot%20from%202018-08-24%2011-07-30.png)\n",
    "\n",
    "![Screenshot%20from%202018-08-24%2011-09-52.png](Screenshot%20from%202018-08-24%2011-09-52.png)\n",
    "\n",
    "#### Haar-like Features:\n",
    "derived from `haar wavelets`<br>\n",
    "The features are `edge features,line features and four rectangle features`<br>\n",
    "<b>fig. haar like features</b>\n",
    "![Screenshot%20from%202018-08-24%2011-13-46.png](Screenshot%20from%202018-08-24%2011-13-46.png)\n",
    "\n",
    "![Screenshot%20from%202018-08-24%2011-17-52.png](Screenshot%20from%202018-08-24%2011-17-52.png)\n",
    "\n",
    "The Viola-Jones Algorithm uses haar like features to detect the features.<br>\n",
    "For example lets consider the nose of the person in the figure. <br> the pixel matrix and look for the `edge feature`, comparing with the edge feature from haar like features we see that 0.0 is white and 1.0 is black, now in picture(grey scale) we get the points and say we calculate the average of the `light` part and `dark` part and then calculate the difference between, if the difference is surpass the `threshold` given in the algorithm `0.3` then we can consider that we detected the haar like edge.<br>\n",
    "<b>Fig. Haar like edge detection example</b>\n",
    "![Screenshot%20from%202018-08-24%2011-20-38.png](Screenshot%20from%202018-08-24%2011-20-38.png)\n",
    "\n",
    "#### Integral Image:\n",
    "The above step to detect the haar like features can be computationally very big.<br>\n",
    "To make it less complex we use `Integral Image`.<br>\n",
    "Every pixel in the integral image is the sum of the pixels that are in the rectangle that this pixel makes. This makes us very easy to find the sums to detect the haar like features than what it would be in the raw image.<br>\n",
    "<b>Fig. Using Integral images to decrease the complexity of haar like feature detection.</b>\n",
    "![Screenshot%20from%202018-08-24%2011-40-41.png](Screenshot%20from%202018-08-24%2011-40-41.png)\n",
    "\n",
    "\n",
    "### Training:\n",
    "Train on the features to determine the features and set the threshold to detect something as a feature.<br>\n",
    "We scale the image down to `24*24` pixels and then try to detect the features using haar like features. or we do it other way round, i.e we scale the features up to the size of image.<br>\n",
    "Thus we train on lots of faces. And also on pictures without any faces as then it will be able to train itself and decrease the false positives.<br>\n",
    "<b>Figures</b>\n",
    "![Screenshot%20from%202018-08-24%2015-31-00.png](Screenshot%20from%202018-08-24%2015-31-00.png)\n",
    "![Screenshot%20from%202018-08-24%2015-32-26.png](Screenshot%20from%202018-08-24%2015-32-26.png)\n",
    "![Screenshot%20from%202018-08-24%2015-25-42.png](Screenshot%20from%202018-08-24%2015-25-42.png)\n",
    "\n",
    "\n",
    "### Adaptive Boosting(Adaboost):\n",
    "Even in `24*24` pixels picture the number of features is over 180,000 so on huge dataset the training will become computationally very hard.<br>\n",
    "`Adaboost`:<br>\n",
    "We take our image and put it in the classifier, each of the features are individually `weak classifiers` with the `ensemble method` we determine the strong classifier.<br>\n",
    "\n",
    "\n",
    "In order to find out the most important features, we use `adaboost`. say one feature detected faces in following way: with false positives and false negatives, true positives and true negatives:<br>\n",
    "\n",
    "\n",
    "\n",
    "Adaboost---> will help it decrease the false negatives and false positives(some other feature will work on the area that this feature was weak on to improve the performance. i.e the other feature will be the one that best compliments the selected feature.)<br>\n",
    "Hence in next round of training it will give more weights to where the errors were made.ie. the importance of the the errors for next haar like feature detection becomes more.<br>\n",
    "<b>Figures</b>\n",
    "![Screenshot%20from%202018-08-24%2015-53-12.png](Screenshot%20from%202018-08-24%2015-53-12.png)\n",
    "![Screenshot%20from%202018-08-24%2015-54-12.png](Screenshot%20from%202018-08-24%2015-54-12.png)\n",
    "\n",
    "\n",
    "### Cascading:\n",
    "We select a subwindow and then try one of the most important features on it, if the feature is not present, we discard the sub window.<br>\n",
    "However if there is the feature present in the the subwindow we check it for the next important feature.<br>\n",
    "This way we proceed...<br>\n",
    "Infact while looking for the features, it looks for the top 5(say) features at a time in reality as dropping a subwindow just cuz we were not able to detect one feature can be risky.<br>\n",
    "![Screenshot%20from%202018-08-24%2016-00-54.png](Screenshot%20from%202018-08-24%2016-00-54.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library import \n",
    "import cv2\n",
    "\n",
    "#Loading Cascades\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "smile_cascade = cv2.CascadeClassifier('haarcascade_smile.xml')\n",
    "\n",
    "#Detecting features\n",
    "def detect(gray,frame):\n",
    "    face = face_cascade.detectMultiScale(gray,1.3,5)\n",
    "    for (x,y,w,h) in face:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h,x:x+w]\n",
    "        roi_color = frame[y:y+h,x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray,1.1,22)\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "        smile = smile_cascade.detectMultiScale(roi_gray,1.7,22)\n",
    "        for (sx,sy,sw,sh) in smile:\n",
    "            cv2.rectangle(roi_color,(sx,sy),(sx+sw,sy+sh),(0,0,255),2)\n",
    "    return frame\n",
    "\n",
    "#deploying webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    _,frame = video_capture.read()\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    canvas = detect(gray,frame)\n",
    "    cv2.imshow('Video',canvas)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![Screenshot%20%2849%29.png](Screenshot%20%2849%29.png)\n",
    "![Screenshot%20%2852%29.png](Screenshot%20%2852%29.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
