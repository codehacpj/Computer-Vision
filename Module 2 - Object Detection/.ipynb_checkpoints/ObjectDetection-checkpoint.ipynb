{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object detection\n",
    "1. Single shot detection\n",
    "2. Multibox concept\n",
    "3. predicting object positions\n",
    "4. The scale problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single shot detection:\n",
    "Rather than going about using CNNs to determine the box where the object could be, the `boxes` go once through the algorithm to detect the gradient difference inorder to determine the most probable chances of finding the object.<br>\n",
    "![Screenshot%20from%202018-08-25%2011-43-10.png](Screenshot%20from%202018-08-25%2011-43-10.png)\n",
    "\n",
    "<b>Figure: The paper</b>\n",
    "![Screenshot%20from%202018-08-25%2011-45-48.png](Screenshot%20from%202018-08-25%2011-45-48.png)\n",
    "\n",
    "## Multibox concept:\n",
    "To train an SSD we need `ground truth` SSD will break image and divide it in the boxes:<b>fig</b>\n",
    "![Screenshot%20from%202018-08-25%2012-24-32.png](Screenshot%20from%202018-08-25%2012-24-32.png)\n",
    "![Screenshot%20from%202018-08-25%2012-24-58.png](Screenshot%20from%202018-08-25%2012-24-58.png)![Screenshot%20from%202018-08-25%2012-32-27.png](Screenshot%20from%202018-08-25%2012-32-27.png)\n",
    "\n",
    "\n",
    "For every single boxes it will try to find the class of object that it was trained for.<b>fig</b>\n",
    "\n",
    "First of all the boxes will determine if the object is present or not using algorithm and then compare to `ground truth`, now the error will be calculated and backpropagated inside the network to adjust the weights to better address the error. All the boxes can be considered as a separate image which is working on to detect the object using `CNN`.<br>\n",
    "<b>fig: ssd in work(multibox concept)above</b>\n",
    "\n",
    "\n",
    "\n",
    "## Predicting Object Positions:\n",
    "After the object is detected with matching features, the position of objects have to be determined, without knowing the full part of the the image we need to find the object position. After the detection is complete, the rectangles will be compared to the ground truth rectangle and then the errors will backpropagate to adjust the weights of boxes and finally we will reach to the full detection of boat.<br>\n",
    "<b>Fig</b>\n",
    "![Screenshot%20from%202018-08-25%2012-36-06.png](Screenshot%20from%202018-08-25%2012-36-06.png)\n",
    "\n",
    "\n",
    "## Scale Problem:\n",
    "Here for example <b>fig of horse detection example</b>:\n",
    "![Screenshot%20from%202018-08-25%2012-44-28.png](Screenshot%20from%202018-08-25%2012-44-28.png)![Screenshot%20from%202018-08-25%2012-53-05.png](Screenshot%20from%202018-08-25%2012-53-05.png)\n",
    "\n",
    "Here the horse in the front is missed by the algorithm as its `scale` is too big and thus the algorithm can't detect this horse.<br>\n",
    "\n",
    "To deal with the scale problem, the image goes through resizing while going through the convolutions every part of the convolution has smaller form of the image to work on. After the resizing the same algorithm will run to detect the objects once again and finally, every layer will have the information about how to get back to the original size and detect this object in actual image.<br>\n",
    "\n",
    "Through continous training process, it will find the objects with certain accuracy.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. libraries\n",
    "2. getting the frame and then converting the frame into torch array---> for it to be compatible with the neural network <br> a. transform to make sure it has right dimension and format. <br> b. conversion to numpy array to torch tensor.<br> c. third transform to add fake dimention which will correspond to batch.<br> d. Convert the torch tensor into torch Variable(torch.autograd-->Variable.).<br>\n",
    "\n",
    "\n",
    "3. convert to torch tensor, with torch.from_numpy and permute it from red blue green to geen red blue\n",
    "\n",
    "4. As neural network only takes images into batches and thus we have to add new dimention to accomodate the batches----> using unsqueeze() function takes index of the dimention and batch should only have the first dimention so `0`. \n",
    "\n",
    "5. Variableis used to convert it to torch variable.\n",
    "6. feed the neural network in the network(pretrained nerual network).\n",
    "7. now from the above output we get .data attribute to get the detections tensor.\n",
    "8. the position of the detected object inside the image has to be normalized between 0 and 1. for this we will create the scale tensor object with width height width height. First two wil correspond to the upper left corner of the rectangle detector and second two will correspond to the lower right corner of this rectangle detector. Done by torch.Tensor().\n",
    "\n",
    "here the detection tensor has batch, number of classes, number of occurence of the class, tuple (score,x0,y0,x1,y1) score is like the threshold for the occurence and x0,y0 lower left corner, x1,y1 upper right corner. Detection tensor above has all these as elements."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7. we iterate for the all the classes and then get score for all these classes then determine the if the object is there in the frame or not as implied by the threshold. detections.size(1) will give the number of classes.\n",
    "8. again for every object with score greater than 0.6 we calculate:\n",
    "9. we keep them and scale them multiplying them using the scale from above.\n",
    "10. we draw the rectangle on detected object\n",
    "11. lets add the label to the detected object\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "12. Buid ssd function from ssd.py to get the pretrained model \n",
    "13. on the above object call load_state_dict to get the weights on tensor\n",
    "14. To make the image compatible with the neural network, we use BaseTransform to transform it.\n",
    "\n",
    "15. To detect the objects from the video, open the video using imageio.get_reader()\n",
    "\n",
    "16. Get the fps using .get_meta_data() on above reader\n",
    "\n",
    "17. create the output video with .get_writer()\n",
    "\n",
    "18. on every frame of the video we process it using detect function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework Solution\n",
    "\n",
    "# Importing the libraries\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "from data import BaseTransform, VOC_CLASSES as labelmap\n",
    "from ssd import build_ssd\n",
    "import imageio\n",
    "\n",
    "# Defining a function that will do the detections\n",
    "def detect(frame, net, transform):\n",
    "    height, width = frame.shape[:2]\n",
    "    frame_t = transform(frame)[0]\n",
    "    x = torch.from_numpy(frame_t).permute(2, 0, 1)\n",
    "    x = Variable(x.unsqueeze(0))\n",
    "    y = net(x)\n",
    "    detections = y.data\n",
    "    scale = torch.Tensor([width, height, width, height])\n",
    "    # detections = [batch, number of classes, number of occurence, (score, x0, Y0, x1, y1)]\n",
    "    for i in range(detections.size(1)):\n",
    "        j = 0\n",
    "        while detections[0, i, j, 0] >= 0.6:\n",
    "            pt = (detections[0, i, j, 1:] * scale).numpy()\n",
    "            cv2.rectangle(frame, (int(pt[0]), int(pt[1])), (int(pt[2]), int(pt[3])), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, labelmap[i - 1], (int(pt[0]), int(pt[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            j += 1\n",
    "    return frame\n",
    "\n",
    "# Creating the SSD neural network\n",
    "net = build_ssd('test')\n",
    "net.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth', map_location = lambda storage, loc: storage))\n",
    "\n",
    "# Creating the transformation\n",
    "transform = BaseTransform(net.size, (104/256.0, 117/256.0, 123/256.0))\n",
    "\n",
    "# Doing some Object Detection on a video\n",
    "reader = imageio.get_reader('epic_horses.mp4')\n",
    "fps = reader.get_meta_data()['fps']\n",
    "writer = imageio.get_writer('output.mp4', fps = fps)\n",
    "for i, frame in enumerate(reader):\n",
    "    frame = detect(frame, net.eval(), transform)\n",
    "    writer.append_data(frame)\n",
    "    print(i)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot%20%2858%29.png](Screenshot%20%2858%29.png)\n",
    "![Screenshot%20%2857%29.png](Screenshot%20%2857%29.png)\n",
    "![Screenshot%20%2859%29.png](Screenshot%20%2859%29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above are the screen shots from the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
